---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.14.1
  kernelspec:
    display_name: R
    language: R
    name: ir
---

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-cd3d46f4e6f33165"} -->
# Tutorial 10: Errors in Inference

From this section, students are expected to be able to:

- Define type I & II errors.
- Describe responsible use and reporting of p-values from hypothesis tests.
- Discuss how these errors are linked to a "reproducibility crisis".
- Measure how these errors amplify when performing multiple hypothesis testing, in the context of multiple comparisons.
<!-- #endregion -->

```{r nbgrader={'schema_version': 3, 'solution': False, 'grade': False, 'locked': True, 'task': False, 'grade_id': 'cell-3e6f826e36bc00c4'}}
# Run this cell before continuing.
library(tidyverse)
library(datateachr)
library(repr)
library(digest)
library(infer)
library(digest)
library(testthat)
source("tests_tutorial_10.R")
```

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-aa6b4b73e649d08e"} -->
## 1. Type of Errors
<!-- #endregion -->

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-5a4826c50d50879e"} -->
**Question 1.1**
<br>{points: 3}

Pfizer, a pharmaceutical company, wants to develop a diagnostic test for hemochromatosis, a genetic condition where the body accumulates iron. They suspect that people with this condition end up producing considerably less transferrin (a protein that binds to iron) than healthy patients. To investigate this claim, they want to conduct a hypothesis test. 

Identify Type I and Type II Errors in this case.
<!-- #endregion -->

<!-- #region nbgrader={"schema_version": 3, "solution": true, "grade": true, "locked": false, "task": false, "points": 3, "grade_id": "cell-cd5e7e776e4dc695"} -->

### BEGIN SOLUTION
- **Type I Error**: to conclude that the transferrin levels of patients with hemochromatosis are significantly **lower** than those of healthy people when, in fact, they are the same. 

- **Type II Error**: conclude that the transferrin levels of patients with hemochromatosis are not significantly **lower** from those of healthy people when, in fact, they are **lower**. 

### END SOLUTION
<!-- #endregion -->

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-f37f4472c8cd0da9"} -->
**Question 1.2**
<br>{points: 1}

After careful investigation, Pfizer concluded that transferrin levels in people with hemochromatosis are significantly lower than those in healthy people. Pfizer might be committing:

A. Type I Error
 
B. Type II Error

C. None of the above.

_Assign your answer to an object called `answer1.2`. Your answer should be a single character surrounded by quotes._
<!-- #endregion -->

```{r nbgrader={'schema_version': 3, 'solution': True, 'grade': False, 'locked': False, 'task': False, 'grade_id': 'cell-d94b6b90809183fb'}}
# answer1.2 <- ...

### BEGIN SOLUTION
answer1.2 <- "A"
### END SOLUTION
```

```{r nbgrader={'schema_version': 3, 'solution': False, 'grade': True, 'locked': True, 'task': False, 'points': 1, 'grade_id': 'cell-39b9ed7861584b55'}}
# Here we check to see if you have given your answer the correct object name
# and if your answer is plausible. However, all other tests have been hidden
# so you can practice deciding when you have the correct answer.
test_that('Did not assign answer to an object called "answer1.2"', {
  expect_true(exists("answer1.2"))
})
test_that('Solution should be a single character ("A", "B", "C", or "D")', {
  expect_match(answer1.2, "a|b|c", ignore.case = TRUE)
})
### BEGIN HIDDEN TESTS
test_that("Solution is incorrect", {
  expect_equal(digest(tolower(answer1.2)), "127a2ec00989b9f7faf671ed470be7f8")
})
print("Success!")
### END HIDDEN TESTS
```

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-c79101eb75cd6682"} -->
**Question 1.3**
<br>{points: 2}

Suppose the transferrin level in patients with hemochromatosis follows $N(260 \text{ mg/dL}, 35 \text{ mg/dL})$ and in healthy people follows $N(275 \text{ mg/dL}, 35 \text{ mg/dL})$. 

If Pfizer tested $H_0: \mu = 275$ vs $H_1: \mu < 275$, at 5% significance level, using a sample of 30 patients with hemochromatosis, what are:
1. probability of type I error?
2. probability of type II error?
3. the power of the test?

_Assign your data frame to an object called `pfizer_errors`. The data frame should have three columns, `type_I_error`, `type_II_error`, and `power_of_test`, and one row with the corresponding probability._
<!-- #endregion -->

```{r nbgrader={'schema_version': 3, 'solution': True, 'grade': False, 'locked': False, 'task': False, 'grade_id': 'cell-80546cddb54deff4'}}
# n <- 30
# pfizer_errors <- tibble(type_I_error = ...,
#                         type_II_error = ..., 
#                         power_of_test = ...)

### BEGIN SOLUTION
n <- 30
pfizer_errors <- tibble(type_I_error = 0.05,
                        type_II_error = 1-pnorm(qnorm(0.05, 275, 35/sqrt(n)), 260, 35/sqrt(n)), 
                        power_of_test = pnorm(qnorm(0.05, 275, 35/sqrt(n)), 260, 35/sqrt(n)))
### END SOLUTION
head(pfizer_errors)
```

```{r nbgrader={'schema_version': 3, 'solution': False, 'grade': True, 'locked': True, 'task': False, 'points': 2, 'grade_id': 'cell-d9054652fb9ca370'}}
# Here we check to see if you have given your answer the correct object name
# and if your answer is plausible. However, all other tests have been hidden
# so you can practice deciding when you have the correct answer.
test_that('Did not assign answer to an object called "pfizer_errors"', {
  expect_true(exists("pfizer_errors"))
})
test_that("Solution should be a data frame", {
  expect_true("data.frame" %in% class(pfizer_errors))
})

expected_colnames <- c("type_I_error", "type_II_error", "power_of_test")
given_colnames <- colnames(pfizer_errors)
test_that("Data frame does not have the correct columns", {
  expect_equal(length(setdiff(
    union(expected_colnames, given_colnames),
    intersect(expected_colnames, given_colnames)
  )), 0)
})

### BEGIN HIDDEN TESTS
test_that("Data frame does not contain the correct number of rows", {
  expect_equal(digest(as.integer(nrow(pfizer_errors))), "4b5630ee914e848e8d07221556b0a2fb")
})

test_that("Data frame does not contain the correct data", {
  expect_equal(digest(as.integer(sum(pfizer_errors$type_I_error) * 10e6)), "13dff35616f15788a3612d24ae7e23c5")
  expect_equal(digest(as.integer(sum(pfizer_errors$type_II_error) * 10e8)), "9e18a0fbd8fcfc89276d6d0b574a8cc6")
  expect_equal(digest(as.integer(sum(pfizer_errors$power_of_test) * 10e8)), "7585ead8d536129a5d6134ae78690364")
})

print("Success!")
### END HIDDEN TESTS
```

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-dd490d3ba7018fb1"} -->
**Question 1.4**
<br>{points: 2}

Suppose that Pfizer knew that the mean transferrin level in healthy people was $\mu_{healthy} = 275$ and the standard deviation was $\sigma = 35$ for both healthy people and patients with hemochromatosis. They want to reject $H_0: \mu = 275$ in favour of $H_1: \mu < 275$ at least 95% of the times if the difference in transferrin level between healthy people and patients with hemochromatosis is 5 mg/dL or more. Using a 5% significance level, what is the smallest sample size to satisfy this requirement?
<!-- #endregion -->

<!-- #region nbgrader={"schema_version": 3, "solution": true, "grade": true, "locked": false, "task": false, "grade_id": "cell-967317c025522fcc", "points": 2} -->

### BEGIN SOLUTION
First, we will reject $H_0$ if $\bar{X} \leq qnorm\left(0.05, 275, 35\left/\sqrt{n}\right.\right)$.

We want this to happen 95% of the time, if $\mu$ is at 270 or lower. If $\mu = 270$, we have that:

\begin{align}
P\left(\bar{X}\leq 275-qnorm(0.95)\times 35/\sqrt{n}\right)\geq 0.95 & \Rightarrow 270+qnorm(0.95)\times 35/\sqrt{n}\leq 275-qnorm(0.95)\times 35/\sqrt{n}\Rightarrow\\
qnorm(0.95)\times 35/\sqrt{n}\leq 275-270 & \Rightarrow n \geq 531
\end{align}

### END SOLUTION
<!-- #endregion -->

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-94f450fcaaea511b"} -->
## 2. Reporting Hypothesis Tests

Unfortunately, it is widespread for studies to report the p-values only. The p-values are indeed an important aspect of hypothesis tests. However, it does not show you the full picture. Let's take a look in an example \[[Amrhein et al. ](https://www.nature.com/articles/d41586-019-00857-9), 2019; [Schmidt & Rothman](https://www-sciencedirect-com.ezproxy.library.ubc.ca/science/article/pii/S0167527314019251?via%3Dihub), 2014\]. 

Two studies examined one side effect of anti-inflammatory drug on the risk of atrial fibrillation \[[1](https://www-sciencedirect-com.ezproxy.library.ubc.ca/science/article/pii/S0167527314019251?via%3Dihub#bb0010)\]\[[2](https://pdf.sciencedirectassets.com/271057/1-s2.0-S0167527313X00221/1-s2.0-S0167527312011643/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEGIaCXVzLWVhc3QtMSJIMEYCIQDLG3nNNWTq0byPLf0NLthWbCcyNuDzCWoAMQ%2F9aQkFiQIhAOprDZGjYeH3PUgTPSwNZ6yCrjYpeE0ndDM59fmlPEmlKr0DCJr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEQAxoMMDU5MDAzNTQ2ODY1IgySaOWh85l%2BxJu8rHsqkQNB3h3EyPDuVl13%2F9Du%2FDg%2FDUzcMRyfwbua%2FnDvY5m2h0nWhtPXTjVxxNxhHu1qL%2BsOiYPDmXjtYZAYt7HwpEIKdFXVV1Tv6tmnt88c9CsnhRtUoPvAk%2FUOEHDdlzxRzSu5btGqNCjlygWRA4jdtwrlwfzPZu8iwhlRfXyIlbXoJbX9zVAgTrVbSCqZ7pL6Za8HapkeQkU%2FebPztAuxaekqxgcqmNt697Yzsp4V%2FRop5lw03A7H4z0f71kv7MZIAiHkA8WxXKhqyghGB9oTdqf90iFZ8Ds5QUgg9qJuu74jE9tVWho3Nex1gjFD2WDmRhz6FPSXBcnv%2Bz9HJ5Ssw9ggVWk3BdmioUKTiLuSwV4ekkNNyY98Vjd8jEXqdXyYy9iVuNqu4HV5ETgwfniMULLZSoogGIUXnHVJMacDASbUTk2MUit5m3fsoTuSV2ugNNmfl2oLuyf5GdyTv2lMbEA1SHOaW%2FdyssCTofEZIaV%2BI1F8NtyeWzSp7fKGutK1qwBSuXhK0SdSPG4kM2ff73rW8zDR3t%2BCBjrqAaxwlUxDzLKSWMq%2BnGuMDwCQwNdByc00Y1DSL%2BlJcwfkENDkQzKcjAF%2FVkyqbpdQyKDsVYOGHxunR2Og9uXfw0KwVM0HRSTfrrnP1TybUi6en0X2Uu%2FirleIWqlYEs%2F1fAiOk%2Be25pRMKAyc3zhiq%2F0aguLaEF3pK8x6KBnmMoJplmFQVmdZy0K58E9fQtZox3KPMFlFXhlvLHo4OlSNxRui8AWjb8cP1qKoDSQ65xdw6RI4PI44g5HvH0v%2FFPYZum8yhZ%2FzcpAMxUa80b5CVqAsPzAL9bP6q937MIWgbXFYUE2qmIbfcqZ1vw%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210322T021532Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYRKWCAGCO%2F20210322%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=c0513e582aab8bd823f30688eb1e857e312254f0eaea55ef4aa71aa09ef57dde&hash=f62f713f16bf5dba6f1eb34391b4b905375ca55be15a9b5297e147132cc639a4&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0167527312011643&tid=spdf-5e1ff066-4996-4f49-a0a5-31f941d616b0&sid=31e6d1e96e8fd8482638b5505eaa89d801e5gxrqa&type=client)\]. The comparison is given by a ratio of chances, therefore, a ratio of 1 means that both groups (people exposed to anti-inflammatory and people not exposed) have the same chance of atrial fibrillation. On the other hand, a ratio higher than 1, say 1.2, means that people exposed to anti-inflammatory drugs have a 20% more chance of atrial fibrillation. The Schmidt et al's study obtained a p-value of 0.0003, while the Chao et al's study obtained a p-value of 0.091.
<!-- #endregion -->

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-a96369c5dc7404af"} -->
**Question 2.1**
<br>{points: 1}

True or false?

 Based on these p-values, at 5% significance, Schmidt et al's study rejected the null hypothesis of the same risk for both groups in favour of $H_1$, while Chao et al's study did not reject the null hypothesis.

_Assign your answer to an object called `answer2.1`. Your answer should be either "true" or "false", surrounded by quotes._
<!-- #endregion -->

```{r nbgrader={'schema_version': 3, 'solution': True, 'grade': False, 'locked': False, 'task': False, 'grade_id': 'cell-ab71a8ed0a5b9715'}}
# answer2.1 <- ...

### BEGIN SOLUTION
answer2.1 <- "true"
### END SOLUTION
```

```{r nbgrader={'schema_version': 3, 'solution': False, 'grade': True, 'locked': True, 'task': False, 'points': 1, 'grade_id': 'cell-69fc945698a30eee'}}
# Here we check to see if you have given your answer the correct object name
# and if your answer is plausible. However, all other tests have been hidden
# so you can practice deciding when you have the correct answer.
test_that('Did not assign answer to an object called "answer2.1"', {
  expect_true(exists("answer2.1"))
})
test_that('Answer should be "true" or "false"', {
  expect_match(answer2.1, "true|false", ignore.case = TRUE)
})
### BEGIN HIDDEN TESTS
test_that("Solution is incorrect", {
  expect_equal(digest(tolower(answer2.1)), "05ca18b596514af73f6880309a21b5dd")
})
print("Success!")
### END HIDDEN TESTS
```

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-d8269605c1d7dadc"} -->
Looking further, we have the following additional information:

Study | Observed risk ratio | Confidence interval | p-value | claim
------|---------------------|---------------------|---------|------
Schmidt et al | 1.2 | \[1.09; 1.33\] | 0.0003 | significant risk difference
Chao et al | 1.2 | \[0.97; 1.48\] | 0.091 | no-significant risk difference

As you can see from the table above, both studies presented the exact same observed risk. In addition, although the confidence interval of Chao et al's study contains 1 (hence the non-significant conclusion), the interval goes from a mere 3% below 1 to a considerable 48% above 1 (which means a 48% higher risk). By looking at the complete picture, the studies seem to be corroborating each other instead of contradicting each other. The difference in the conclusion of the hypothesis test is due to the fact that Schmidt et al's study had a much higher precision (compare the length of the confidence intervals).
<!-- #endregion -->

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-15b5f5571842e179"} -->
**Question 2.2**
<br>{points: 1}

True or false?

The problem is that, by only reporting the p-value, we are missing information on (select all that apply):

A. the observed effect size;

B. the error associated with the statistic;

C. the ability to decide if $H_0$ is rejected or not for a given significance level;

_Assign your answer to an object called `answer2.2`. Your answer should be a sequence of characters surrounded by quotes._
<!-- #endregion -->

```{r nbgrader={'schema_version': 3, 'solution': True, 'grade': False, 'locked': False, 'task': False, 'grade_id': 'cell-e336abc330ae2835'}}
# answer2.2 <- ...

### BEGIN SOLUTION
answer2.2 <- "AB"
### END SOLUTION
```

```{r nbgrader={'schema_version': 3, 'solution': False, 'grade': True, 'locked': True, 'task': False, 'points': 1, 'grade_id': 'cell-87da139c65cf88e5'}}
# Here we check to see if you have given your answer the correct object name
# and if your answer is plausible. However, all other tests have been hidden
# so you can practice deciding when you have the correct answer.
test_that('Did not assign answer to an object called "answer2.2"', {
  expect_true(exists("answer2.2"))
})

### BEGIN HIDDEN TESTS
test_that("Solution is incorrect", {
  expect_equal(digest(tolower(paste(sort(unlist(strsplit(answer2.2, ""))), collapse = ""))), "8b63e49106226d2a45958a7e24c97c37")
})
print("Success!")
### END HIDDEN TESTS
```

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-20828d9466f32ac4"} -->
This example illustrates the fundamental importance of reporting all the quantities associated with the hypothesis tests: (1) observed test statistic; (2) measurement of error, either a confidence interval or standard error; (3) p-value.  By providing all these quantities, we are able to properly contrast multiple studies and make better-informed conclusions about the significance of the observed differences. 
<!-- #endregion -->

<!-- #region nbgrader={"schema_version": 3, "solution": false, "grade": false, "locked": true, "task": false, "grade_id": "cell-b6d779e1dab4bcf0"} -->
**Question 2.3**
<br>{points: 1}

As the last exercise, read these two articles:

1. [[Amrhein et al., 2019;](https://www.nature.com/articles/d41586-019-00857-9)\]. 
2. [ASA Statement on Statistical Significance and P-Values](https://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108#.YFgHX0NKho_)




True or false?

I read both articles. 

_Assign your answer to an object called `answer2.3`. Your answer should be either "true" or "false", surrounded by quotes._
<!-- #endregion -->

```{r nbgrader={'schema_version': 3, 'solution': True, 'grade': False, 'locked': False, 'task': False, 'grade_id': 'cell-4f12f2798b37b17c'}}
# answer2.3 <- ...

### BEGIN SOLUTION
answer2.3 <- "true"
### END SOLUTION
```

```{r nbgrader={'schema_version': 3, 'solution': False, 'grade': True, 'locked': True, 'task': False, 'points': 1, 'grade_id': 'cell-b0cf3ada82fb1352'}}
test_that('Did not assign answer to an object called "answer2.3"', {
    expect_true(exists("answer2.3"))
})

test_that('Solution should be "true" or "false"', {
    expect_match(answer2.3, "true|false", ignore.case = TRUE)
})

answer_hash <- digest(tolower(answer2.3))
if (answer_hash == "d2a90307aac5ae8d0ef58e2fe730d38b") {
    print("You really should read these articles. :) ")
}

test_that("Solution is incorrect", {
    expect_equal(answer_hash, "05ca18b596514af73f6880309a21b5dd")
})

print("Success!")

```

<!-- #region nbgrader={"grade": false, "grade_id": "cell-8b42616ac3e9c479", "locked": true, "schema_version": 3, "solution": false, "task": false} -->
## 4. Simulation: Exploring the power of a test

In this section, we will simulate data to illustrate two concepts related to hypothesis test. 


Suppose that we are trying to test if the level of albumin in patients with fibrosis is 44g/L (which is the mean albumin level of patients with healthy liver): 

$H_0: \mu = 44g/L$ vs $H_1:\mu < 44g/L$. 

Let's assume that the true mean of patients with Fibrosis is 41.76. Therefore, we know that the alternative hypothesis $H_1$ is the true one. Let's simulate a population now. 
<!-- #endregion -->

```{r nbgrader={'grade': False, 'grade_id': 'cell-4058aae31601537d', 'locked': True, 'schema_version': 3, 'solution': False, 'task': False}}
set.seed(1)
# Run this cell before continuing
true_pop <- 
    tibble(albumin = rnorm(50000, 41.76, 3.74))

head(true_pop)
```

<!-- #region nbgrader={"grade": false, "grade_id": "cell-b1e0fca5fb5cf275", "locked": true, "schema_version": 3, "solution": false, "task": false} -->
**Question 4.1**
<br>{points: 3}

Fill in the code below to plot the histogram of the true population.

_Assign your answer to an object called `true_pop_hist`._
<!-- #endregion -->

```{r nbgrader={'grade': False, 'grade_id': 'cell-5277c893ea57b610', 'locked': False, 'schema_version': 3, 'solution': True, 'task': False}}

#true_pop_hist <- 
#    true_pop %>% 
#    ....() +
#    ...(..., color="#FF9999", fill="#FF9999", alpha=.35, bins=55)+
#    theme(text = element_text(size=20))

### BEGIN SOLUTION
true_pop_hist <- 
    true_pop %>% 
    ggplot() +
    geom_histogram(aes(albumin), color="#FF9999", fill="#FF9999", alpha=.35, bins=55)+
    theme(text = element_text(size=20))
### END SOLUTION

true_pop_hist
```

```{r nbgrader={'grade': True, 'grade_id': 'cell-ff9469396bea4423', 'locked': True, 'points': 3, 'schema_version': 3, 'solution': False, 'task': False}}
test_4.1()
```

<!-- #region nbgrader={"grade": false, "grade_id": "cell-4a8886c0ab0b4ba6", "locked": true, "schema_version": 3, "solution": false, "task": false} -->
**Question 4.2**
<br>{points: 3}

To test the hypothesis, we start by taking a sample (from the true population) and calculating the test statistic. In this exercise, you are going to use `rep_sample_n` to take one sample of size 30. 

_Assign your answer to an object called `sample`._
<!-- #endregion -->

```{r nbgrader={'grade': False, 'grade_id': 'cell-29b335c916d997f7', 'locked': False, 'schema_version': 3, 'solution': True, 'task': False}}
set.seed(1) # Do not change this.

#sample <- 
#    true_pop %>% 
#    ...(... = 30)

### BEGIN SOLUTION
sample <- 
    true_pop %>% 
    rep_sample_n(size = 30)
### END SOLUTION

head(sample)
```

```{r nbgrader={'grade': True, 'grade_id': 'cell-b050d4afb4c8b839', 'locked': True, 'points': 3, 'schema_version': 3, 'solution': False, 'task': False}}
test_4.2()
```

<!-- #region nbgrader={"grade": false, "grade_id": "cell-8e930442a99a9646", "locked": true, "schema_version": 3, "solution": false, "task": false} -->
**Question 4.3**
<br>{points: 6}

Before we obtain the bootstrap sampling distribution assuming $H_0$ was true (i.e., the null model), let's study the actual bootstrap sampling distribution. Generate 10,000 values from the bootstrap sampling distribution and plot its histogram. 

_Assign the generated values to a tibble object called `boot_sampling_dist` and the histogram to an object called `boot_sampl_dist_hist`._
<!-- #endregion -->

```{r nbgrader={'grade': False, 'grade_id': 'cell-3273b452290bdea4', 'locked': False, 'schema_version': 3, 'solution': True, 'task': False}}
set.seed(1)

#boot_sampling_dist <- 
#    sample %>% 
#    specify(...) %>% 
#    generate(...) %>% 
#    ...(stat = "mean") %>% 
#    mutate(samp_dist = "Actual")

#boot_sampl_dist_hist <- 
#    sampling_dist %>% 
#    ggplot() +
#    geom_histogram(aes(..., color=samp_dist, fill=samp_dist), alpha=.35, bins=20) +
#    theme(text = element_text(size=20)) + 
#    xlab("Albumin g/L")

### BEGIN SOLUTION
boot_sampling_dist <- 
    sample %>% 
    specify(response = albumin) %>% 
    generate(reps = 10000, type = "bootstrap") %>% 
    calculate(stat = "mean") %>% 
    mutate(samp_dist = "Actual")

boot_sampl_dist_hist <- 
    boot_sampling_dist %>% 
    ggplot() +
    geom_histogram(aes(stat, color=samp_dist, fill=samp_dist), alpha=.35, bins=20) +
    theme(text = element_text(size=20)) + 
    xlab("Albumin g/L")
### END SOLUTION

boot_sampl_dist_hist
```

```{r nbgrader={'grade': True, 'grade_id': 'cell-f9503f077ef82acf', 'locked': True, 'points': 3, 'schema_version': 3, 'solution': False, 'task': False}}
test_4.3A()
```

```{r nbgrader={'grade': True, 'grade_id': 'cell-106ccc9fa48f1350', 'locked': True, 'points': 3, 'schema_version': 3, 'solution': False, 'task': False}}
test_4.3B()
```

<!-- #region nbgrader={"grade": false, "grade_id": "cell-7c465ae78a0cb4fe", "locked": true, "schema_version": 3, "solution": false, "task": false} -->
**Question 4.4**
<br>{points: 4}

Next, it is time to conduct the hypothesis test. In this exercise, you need to generate values from the bootstrap sampling distribution under $H_0$ (i.e., from the null model). 

_Assign the test statistic to an object called `obs_test_stat_4.4` and ensure these values are in a tibble named `null_model_4.4`._
<!-- #endregion -->

```{r nbgrader={'grade': False, 'grade_id': 'cell-7beefda387e9c9ec', 'locked': False, 'schema_version': 3, 'solution': True, 'task': False}}
set.seed(1) # Do not change this.

#obs_test_stat_4.4 <- ....

#null_model_4.4 <-
#    sample %>% 
#    specify(...) %>% 
#    ...(...) %>% 
#    generate(reps = 10000, type = "bootstrap") %>% 
#    calculate(...) %>% 
#    mutate(samp_dist = "null model")

### BEGIN SOLUTION
obs_test_stat_4.4 <- mean(sample$albumin)

null_model_4.4 <-
    sample %>% 
    specify(response = albumin) %>% 
    hypothesise(null = "point", mu = 44) %>% 
    generate(reps = 10000, type = "bootstrap") %>% 
    calculate(stat = "mean") %>% 
    mutate(samp_dist = "null model")
### END SOLUTION

obs_test_stat_4.4
head(null_model_4.4)
```

```{r nbgrader={'grade': True, 'grade_id': 'cell-e85fc1a0f7e3ce8f', 'locked': True, 'points': 1, 'schema_version': 3, 'solution': False, 'task': False}}
test_4.4A()
```

```{r nbgrader={'grade': True, 'grade_id': 'cell-a25565c89da9448f', 'locked': True, 'points': 3, 'schema_version': 3, 'solution': False, 'task': False}}
test_4.4B()
```

<!-- #region nbgrader={"grade": false, "grade_id": "cell-0b2a48b3b333b7bb", "locked": true, "schema_version": 3, "solution": false, "task": false} -->
**Question 4.5**
<br>{points: 1}

What type of error are we subject to commit in this hypothesis test?

A. Type I Error

B. Type II Error


_Assign your answer to an object called `answer4.5`. Your answer should be a single character surrounded by quotes._
<!-- #endregion -->

```{r nbgrader={'grade': False, 'grade_id': 'cell-fc7b0870d8bc373c', 'locked': False, 'schema_version': 3, 'solution': True, 'task': False}}
#answer4.5 <- 
### BEGIN SOLUTION
answer4.5 <- "B"
### END SOLUTION

answer4.5
```

```{r nbgrader={'grade': True, 'grade_id': 'cell-3a6b302efd1cb88c', 'locked': True, 'points': 1, 'schema_version': 3, 'solution': False, 'task': False}}
test_4.5()
```

<!-- #region nbgrader={"grade": false, "grade_id": "cell-46032ae9773b2580", "locked": true, "schema_version": 3, "solution": false, "task": false} -->
**Question 4.6**
<br>{points: 1}

To understand how type II error can occur, contrast the null model and the actual sampling distribution shown below:
<!-- #endregion -->

```{r nbgrader={'grade': False, 'grade_id': 'cell-a50901cf0188b6b6', 'locked': True, 'schema_version': 3, 'solution': False, 'task': False}}
boot_sampling_dist %>% 
    bind_rows(null_model_4.4) %>% 
    ggplot() + 
    geom_histogram(aes(stat, color=samp_dist, fill=samp_dist), alpha=.25, bins=40, position="identity" ) +
    theme(text = element_text(size=20)) + 
    xlab("Albumin g/L")
```

<!-- #region nbgrader={"grade": false, "grade_id": "cell-046df4629f84bb79", "locked": true, "schema_version": 3, "solution": false, "task": false} -->
As you can notice from the image above, there is an overlap between the null model and the actual sampling distribution. The bigger this overlap is, the higher is the chance that the actual sampling distribution generates a test statistic in the region of non-rejection of $H_0$ (imagine moving the red distribution towards the right). Unfortunately, for a given sample size, we cannot reduce the probability of type II error without increasing the probability of type I error. 

There are mainly two things that affect the probability of type II error: (1) the effect size (i.e., the difference between the true parameter and the hypothesized value in $H_0$); and (2) the sample size. 

We say that a test is powerful if it has a high chance of detecting when $H_0$ is false. In this sense, the power of a test is one minus the probability of type II error. 

The question for you here is: 
> Why does increasing the sample size also increase the power of a test?

Which of the following explanations to our question above is correct? 

A. Because it increases the effect size by moving the true sampling distribution and the null model in opposing directions, which reduces their overlap. 

B. Because increasing the sample size makes the sampling distribution narrower, reducing the overlap between the true sampling distribution and the null model. 

To help you answer this question, see the plot below. 

_Assign your answer to an object called answer4.6_
<!-- #endregion -->

```{r nbgrader={'grade': False, 'grade_id': 'cell-b506d7e9b291ccc1', 'locked': True, 'schema_version': 3, 'solution': False, 'task': False}}
set.seed(15)  # Don't worry about this code. You can safely ignore it. 

example <-
    tibble(effect_size = c(1,5,9)) %>% 
    mutate(pop = map(effect_size, ~rnorm(2000, 44-.x, 3))) %>% 
    crossing(tibble(n = c(40, 80, 500))) %>% 
    mutate(samp = map2(pop, n, ~tibble(var=sample(.x, size = .y)))) %>%
    select(-pop) %>% 
    mutate(observed_test_stat = map_dbl(samp, ~mean(.x$var))) %>% 
    mutate(null_model = map(samp,
                            ~specify(.x, response=var) %>% 
                            hypothesise(null = "point", mu = 44) %>% 
                            generate(reps = 10000, type='bootstrap') %>% 
                            calculate(stat = "mean") %>% 
                            mutate(samp_dist = "Null Model")),
           samp_dist_temp = map(samp,
                            ~specify(.x, response=var) %>% 
                            generate(reps = 10000, type='bootstrap') %>% 
                            calculate(stat = "mean") %>% 
                            mutate(samp_dist = "Actual"))) %>% 
    mutate(samp_dist = map2(null_model, samp_dist_temp, ~bind_rows(.x,.y))) %>% 
    select(-null_model, -samp_dist_temp, -samp) %>% 
    unnest(samp_dist) 
```

```{r nbgrader={'grade': False, 'grade_id': 'cell-7813ec901bcdc899', 'locked': True, 'schema_version': 3, 'solution': False, 'task': False}}
options(repr.plot.width=15, repr.plot.height=7) # Adjust these number so the plot looks good in your desktop

example %>% 
    ggplot() +    
    geom_histogram(aes(stat, color=samp_dist, fill=samp_dist), alpha=.25, bins=200, position="identity" ) +
    facet_grid(n~effect_size, labeller = labeller(.rows = label_both, .cols = label_both))+
    theme(text = element_text(size=20)) + 
    xlab("Albumin g/L")
```

```{r nbgrader={'grade': False, 'grade_id': 'cell-2b4f17e7473bd754', 'locked': False, 'schema_version': 3, 'solution': True, 'task': False}}
#answer4.6 <-

### BEGIN SOLUTION
answer4.6 <- "B"
### END SOLUTION

answer4.6
```

```{r nbgrader={'grade': True, 'grade_id': 'cell-4b1e027a28698cb7', 'locked': True, 'points': 1, 'schema_version': 3, 'solution': False, 'task': False}}
test_4.6()
```
