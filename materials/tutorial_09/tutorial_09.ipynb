{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "brave-laundry",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c1dfc75fc646113ec836eaa60e38bc8",
     "grade": false,
     "grade_id": "cell-7f7daff5cfa3835f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Tutorial 09: Beyond two-group comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51362373-4a94-4c69-9e17-0b0772ed639e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a00e6170349ecc4e6b637106216c847f",
     "grade": false,
     "grade_id": "cell-c609e089fe44345e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Lecture and Tutorial Learning Goals:\n",
    "After completing this week's lecture and tutorial work, you will be able to:\n",
    "\n",
    "1. Run a simple one-way ANOVA and understand the general concepts behind an ANOVA\n",
    "2. Apply False Detection Rate or Bonferroni correction to control the errors when performing multiple hypothesis testing\n",
    "3. Understand the value of presenting an entire distribution as a prediction\n",
    "4. Calculate and interpret prediction intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-armor",
   "metadata": {
    "deletable": false,
    "editable": false,
    "message": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12b970702f7b420e3d020117d71761e5",
     "grade": false,
     "grade_id": "cell-eaa306f38e77ac65",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "warning": false
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(infer)\n",
    "library(datateachr)\n",
    "library(testthat)\n",
    "library(digest)\n",
    "library(broom)\n",
    "source(\"tests_tutorial_09.R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-saint",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c3bbcad914b25d3f22b87c8041b8a34",
     "grade": false,
     "grade_id": "cell-35ac1699a9df82f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1: Multiple Comparisons\n",
    "\n",
    "Last week, we saw that we're more likely to commit a Type I error as we test more and more hypotheses. This week, we'll investigate two methods for addressing this issue. \n",
    "\n",
    "Consider the GWAS dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-hearing",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab3c77f17a62caa39c27632175353e35",
     "grade": false,
     "grade_id": "cell-e922a43ee98a429e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "gwas <- \n",
    "    read_csv(\"gwas.csv\")  %>% \n",
    "    rename(p_value = `p-value`)\n",
    "\n",
    "head(gwas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-shakespeare",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc5ed6f9396371cc3b34da29d5d1e4e9",
     "grade": false,
     "grade_id": "cell-6467cb7a56a3076f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The dataset, as described by Dr. Tiffany Timbers:\n",
    "\n",
    "> The dataset you will see contains two columns, a list of gene names and a list of unadjusted p-values generated from the analysis (the particular statistical test used is called the Sequence Kernel Analysis test, or SKAT). These p-values were created by repeating the analysis on many variables from the same dataset, thus we have a multiple testing problem to deal with!\n",
    "> \n",
    "> Each p-value corresponds to a gene, and tests whether that gene is associated to a phenotype.\n",
    "> \n",
    "> The study can be accessed here: http://dx.doi.org/10.1371/journal.pgen.1006235\n",
    "\n",
    "In STAT 201, we will very briefly explore two methods for handling multiple hypothesis testing: the __Bonferroni__ and __Benjamin-Hochberg (BH)__ adjustments. Unfortunately, we will not have time to go into how they work. But, the idea for both of them is to increase the p-values so that tests are less likely to pass.\n",
    "\n",
    "In general, it's not always obvious when you should use a p-value adjustment. If you conduct a study that includes a few unrelated hypothesis tests, should you \"proof\" the entire study by adjusting all p-values? Probably not. If you do, and you decide to gain a little bit of extra insight by testing one last hypothesis before submitting the study, should you then adjust all of the p-values in the study, potentially changing your results? This isn't realistic. But, if you can group your hypotheses into one overarching question, like [testing to see which of 20 types of jellybeans cause acne](https://xkcd.com/882/), you then might want to implement some level of error-proofing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-equivalent",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f7785be93e4ac29939c96b70a4e3f48",
     "grade": false,
     "grade_id": "cell-13db9415dfd084a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 1.1**\n",
    "<br>{points: 3}\n",
    "\n",
    "How many of the null hypotheses would be rejected under a 0.05 significance level? Store this value in a variable named `number_significant`. What proportion of the total number of tests is that? Store this value in a variable named `prop_significant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-administrator",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2fa8d55b16f5192116de4b296e20a26",
     "grade": false,
     "grade_id": "cell-bf3ff2835661d424",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# number_significant <- ...\n",
    "# prop_significant <- ...\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "\n",
    "print(number_significant)\n",
    "print(prop_significant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-egyptian",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 2,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9dd3bc53c941d9f10146fc143e18afee",
     "grade": true,
     "grade_id": "cell-04f35cffe3a21414",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Here we check to see if you have given your answer the correct object name\n",
    "# and if your answer is plausible. However, all other tests have been hidden\n",
    "# so you can practice deciding when you have the correct answer.\n",
    "\n",
    "test_that('Did not assign answer to an object called \"number_significant\"', {\n",
    "  expect_true(exists(\"number_significant\"))\n",
    "})\n",
    "\n",
    "test_that('Did not assign answer to an object called \"prop_significant\"', {\n",
    "  expect_true(exists(\"prop_significant\"))\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-extension",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef6d2ca044e1021372d0ea68cfd57eb0",
     "grade": false,
     "grade_id": "cell-c217f340f6c06105",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 1.2: Bonferroni Adjustment**\n",
    "<br>{points: 3}\n",
    "\n",
    "The Bonferroni correction is seen as an aggressive p-value adjustment method. So, it typically won't be able to detect subtle effect sizes. It limits the probability of seeing at least one false positive (Type I error) to the specified significance level $\\alpha$. \n",
    "\n",
    "The idea is, if there are $k$ hypotheses being tested, to either (1) adjust your chosen significance level $\\alpha$ to $\\alpha / k$, or equivalently, (2) multiply your p-values by $k$.\n",
    "\n",
    "Adjust the p-values of the GWAS study according to a Bonferroni adjustment. Store the adjusted p-values in a vector named `pval_bonf`. You can use the `p.adjust()` function, inserting the vector of p-values in the first argument. How many of these null hypotheses are rejected now, under a 0.05 significance level? Put the number in a variable named `count_bonf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-badge",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa9ceeff664bb95326f3fabdfe4a65e8",
     "grade": false,
     "grade_id": "cell-a5ad5a93a0225bbe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# pval_bonf <- p.adjust(..., method = \"bonferroni\")\n",
    "# count_bonf <- ...\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "\n",
    "head(pval_bonf)\n",
    "print(count_bonf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-value",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2381b0e38c1fe456728aa8e6428e5a83",
     "grade": true,
     "grade_id": "cell-d57df8bbcc573872",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Here we check to see if you have given your answer the correct object name\n",
    "# and if your answer is plausible. However, all other tests have been hidden\n",
    "# so you can practice deciding when you have the correct answer.\n",
    "\n",
    "test_that('Did not assign answer to an object called \"pval_bonf\"', {\n",
    "  expect_true(exists(\"pval_bonf\"))\n",
    "})\n",
    "\n",
    "test_that('Did not assign answer to an object called \"count_bonf\"', {\n",
    "  expect_true(exists(\"count_bonf\"))\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-consequence",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5bfc8041865c2e6fcb34cd0074cd551c",
     "grade": false,
     "grade_id": "cell-1f3347800192a83b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 1.3: BH Adjustment**\n",
    "<br>{points: 1}\n",
    "\n",
    "The BH method is a more forgiving method in comparison to the Bonferroni method, and so may be more realistic. Instead of assigning a probability for seeing any Type I error at all, the BH method limits the _false discovery rate_ = the proportion of \"discoveries\" (rejections of the null) that are false. \n",
    "\n",
    "Adjusting the p-values to limit the false discovery rate to some specified level $\\alpha$ is more complicated, and we won't get into details (and won't expect you to, either). But the idea is that, given that p-values are completely random numbers between 0 and 1 _if the null hypothesis is true_, the collection of p-values are compared against what a random sample between 0 and 1 would look like. The p-values that deviate from this pattern are flagged as being significant.\n",
    "\n",
    "Adjust the p-values of the GWAS study according to a BH adjustment. Store the adjusted p-values in a vector named `pval_bh`. How many of these null hypotheses are rejected now, under a 0.05 significance level? Put the number in a variable named `count_bh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-hometown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39d18f2b19264348b4999c32ca77bb4c",
     "grade": false,
     "grade_id": "cell-0d41998b7e66ffee",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# pval_bh <- p.adjust(..., method = \"BH\")\n",
    "# count_bh <- ...\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "\n",
    "head(pval_bh)\n",
    "print(count_bh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-antarctica",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 2,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea2cec3d9d75f4b04585185c6383e908",
     "grade": true,
     "grade_id": "cell-74f008affbe5842d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_1.3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-folks",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f2f99a460661d45704692635335240c",
     "grade": false,
     "grade_id": "cell-033d029a19111ee9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2: Prediction\n",
    "\n",
    "Until now, we've been making inference on _population parameters_, such as means, quantiles, variances, etc. As we've seen, this inference allows us to present both (1) our best guess as to what the parameter is (an estimate), along with (2) a description of uncertainty (such as confidence intervals and hypothesis tests). But, \n",
    "how can we go about making inference on a _new observation_? \n",
    "\n",
    "In the worksheet, you explored the inference part of predictions. In this tutorial, you'll explore the predictions themselves.\n",
    "\n",
    "Consider the survey data stored in `attitude`, used in the worksheet. How can we predict what the survey results would be in another department that wasn't included in the survey? Recall that each row of the dataset corresponds to a department that was surveyed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-shock",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b216736ca6ec9c8549f72446c1ed0e0e",
     "grade": false,
     "grade_id": "cell-5938e2ed2b477949",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "head(attitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-audit",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3859f52fc382987b53d6d46bf57774e8",
     "grade": false,
     "grade_id": "cell-edce3e77046bdf42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As an aside:\n",
    "\n",
    "- This prediction question is what the branch of _supervised learning_ is primarily concerned with. Usually, it's most useful to make predictions when more than one _predictor_ variable is present (for now, we only have one predictor variable: \"question\"). But, without first thinking about how to make predictions in the absence of predictor variables (or at least, in the presence of one grouping variable like we have), it's easy to lose sight of the interpretation of these predictions. (Hint: the interpretation of a prediction is _not_ just \"a best guess\".)\n",
    "- It's worth saying that, if you were working for this organization that was surveyed, you might have anecdotal evidence as to how this department might respond to the survey. Sometimes, in the presence of numeric analyses, it's easy to forget this and to only rely on the numbers, but human input is also very valuable.\n",
    "\n",
    "**Question 2.1**\n",
    "<br>{points: 1}\n",
    "\n",
    "Probably the most common approach to prediction is to use the mean. What would the prediction be for the `\"raises\"` question in this case? Store your prediction in a variable named `prediction_raises`. Yes, this just amounts to calculating the mean!\n",
    "\n",
    "(By the way, it seems most supervised learning models try to make predictions using the mean, although there's nothing about the mean that makes it any better of a prediction than other parameters such as the median!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-receiver",
   "metadata": {
    "deletable": false,
    "lines_to_next_cell": 2,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f08e5cada34785e864ecf0587fb2c12",
     "grade": false,
     "grade_id": "cell-92febe773ef47018",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# prediction_raises <- ...\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "\n",
    "print(prediction_raises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-parameter",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d84025c0b447c367196c99acc8111f7",
     "grade": true,
     "grade_id": "cell-757b068f37a3e5fc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-handle",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac192ae72e68fa13a6fe9512023c2496",
     "grade": false,
     "grade_id": "cell-6a9606c4b9dae453",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2.2**\n",
    "<br>{points: 3}\n",
    "\n",
    "What score would you predict the department to give on the `\"privileges\"` question? This time, use the median as a prediction, using the `median()` function. Store your prediction in the variable `prediction_privileges`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-sampling",
   "metadata": {
    "deletable": false,
    "lines_to_next_cell": 2,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90702b0a133e5734f8d55a412351814d",
     "grade": false,
     "grade_id": "cell-c261a2188ea64431",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# prediction_privileges <- ...\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "\n",
    "print(prediction_privileges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-royalty",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9579fce2b5aa54db90f80baa611b2d9",
     "grade": true,
     "grade_id": "cell-b21efbbae9a28aa6",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Here we check to see if you have given your answer the correct object name\n",
    "# and if your answer is plausible. However, all other tests have been hidden\n",
    "# so you can practice deciding when you have the correct answer.\n",
    "\n",
    "test_that('Did not assign answer to an object called \"prediction_privileges\"', {\n",
    "  expect_true(exists(\"prediction_privileges\"))\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-novel",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c88f5a9613311b4c2cccd30e157c3e05",
     "grade": false,
     "grade_id": "cell-9bbc5ca343f292c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2.3**\n",
    "<br>{points: 1}\n",
    "\n",
    "Depending on the application, you should ask yourself if you truly need a single value prediction. Consider predicting the department's response to the `\"critical\"` question based on the histogram of outcomes below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-moscow",
   "metadata": {
    "deletable": false,
    "editable": false,
    "fig.align": "center",
    "fig.height": 3,
    "fig.width": 5,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5d5028abc1ad3772b5bf0fe6df30dfd",
     "grade": false,
     "grade_id": "cell-8e5594e1925f12c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ggplot(attitude, aes(critical)) +\n",
    "    geom_histogram(bins = 10) +\n",
    "    theme_minimal() +\n",
    "    xlab(\"Score for question 'Critical'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-frequency",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c1f67d0228268bfd542ce60e773b28b",
     "grade": false,
     "grade_id": "cell-3514fdfea77a6f49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Which of these statements can be inferred by looking at the histogram?\n",
    "\n",
    "**a**\\. Probably won't be less than 50   \n",
    "**b**\\. Probably won't be more than 90   \n",
    "**c**\\. Most likely to be somewhere between 65 and 85   \n",
    "**d**\\. All of the above\n",
    "\n",
    "By the way, we're able to come up with this histogram, because we have access to multiple observations for the `\"critical\"` question. In supervised learning scenarios, it's uncommon to have such luxury. Yet, methods exist to access such a distribution -- but that's beyond the scope of this course. \n",
    "\n",
    "Store the letter to your response in a variable named `prediction_critical`. The response should be a single character surrounded by quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-metallic",
   "metadata": {
    "deletable": false,
    "lines_to_next_cell": 2,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c051a410f48020a35e8f90e3b26d5ee2",
     "grade": false,
     "grade_id": "cell-c64a591352da386e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# prediction_critical <- ...\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-feelings",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "410fbe156533581bfe0f02cef662cb5b",
     "grade": true,
     "grade_id": "cell-467db76407515f56",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-emphasis",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ced62b67e06fd53b3943d82e59f48cbf",
     "grade": false,
     "grade_id": "cell-0ac2af721fdb0d8d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 2.4**\n",
    "<br>{points: 1}\n",
    "\n",
    "Ultimately, predictions are useful because they help us make decisions. If you have to make many decisions quickly, it may not be practical to have a human look at a histogram as part of a decision-making procedure. But if there are few decisions that are very important, going beyond a single-value prediction is useful, such as by looking at histograms. \n",
    "\n",
    "Given this, which of the following scenarios **would it be desirable** to investigate histogram(s) before making decisions? Don't worry about how you'd come up with such histograms, or what they'd look like. \n",
    "\n",
    "**a**\\. You are deciding between two people to hire in a high-leverage position in your company, and you want to predict what the \"productivity\" of each person would be after they are hired.    \n",
    "**b**\\. You work for a song streaming company, and want to predict favourable songs for each of your customers automatically.    \n",
    "**c**\\. You are trying to decide whether your business can afford a large purchase.     \n",
    "**d**\\. You work for the Coast Mountain Bus Company, who operates all the city busses in the Vancouver area, and want to predict the demand at every bus stop throughout this coming Canada Day. \n",
    "\n",
    "Put your choices in a single variable named `prediction_decisions`, in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-tumor",
   "metadata": {
    "deletable": false,
    "lines_to_next_cell": 2,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d96b0b1c14d0a015150e3a3bee76c2ff",
     "grade": false,
     "grade_id": "cell-68b20f9ef7b8c69f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# prediction_decisions <- ...\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-unemployment",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 2,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c531d2366e5e8602212c55637025c3cf",
     "grade": true,
     "grade_id": "cell-37c0d2f797452e40",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_2.4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-capacity",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "85884a430312c9f42cf9f484afcb6a2a",
     "grade": false,
     "grade_id": "cell-920af51f0592f3cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3: ANOVA\n",
    "\n",
    "In this exercise, we will revisit the `attitude` survey data from the worksheet. We are going to investigate whether there's a significant difference between the means of two specific questions: \"privileges\" and \"learning\". The following tibble contains the relevant subset of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-howard",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6b9b540e485d6345e9527798df0d10f",
     "grade": false,
     "grade_id": "cell-7d637512e0b77b6a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "attitude_tidy <- \n",
    "    attitude %>% \n",
    "    select(-rating) %>% \n",
    "    pivot_longer(everything(), names_to = \"question\", values_to = \"score\")\n",
    "\n",
    "attitude_two <- \n",
    "    attitude_tidy %>% \n",
    "    filter(question %in% c(\"privileges\", \"learning\"))\n",
    "\n",
    "head(attitude_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-hungary",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ccf9d0d5f2c6515ebeddbbb4acb3c41",
     "grade": false,
     "grade_id": "cell-19ae3432ee0d1878",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 3.1**\n",
    "<br>{points: 1}\n",
    "\n",
    "Test the hypothesis using a two-sample t-test, approximating the group variances to be equal. Use the `tidy()` function from the broom package to tidy your results, putting the resulting tibble in a variable called `two_t_equal_tidy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-custom",
   "metadata": {
    "deletable": false,
    "lines_to_next_cell": 2,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bff2bf585e67d80b049f9a5a65736e7b",
     "grade": false,
     "grade_id": "cell-f94607b905ef0a68",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# two_t_equal_tidy <- \n",
    "#    t.test(... ~ ..., data = attitude_two, \n",
    "#               var.equal = ...) %>% \n",
    "#   tidy()\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "\n",
    "print(two_t_equal_tidy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-repository",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 2,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f89ad7027f60c6bca7f474692883cda",
     "grade": true,
     "grade_id": "cell-8f099f82410d536f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_3.1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-politics",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f6d8eb8433382d28a0cd4f10353f901",
     "grade": false,
     "grade_id": "cell-dbe3ae8b59ed9dd4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 3.2**\n",
    "<br>{points: 1}\n",
    "\n",
    "Test the hypothesis using a two-sample t-test, allowing the group variances to be unequal. Use the `tidy()` function from the broom package to tidy your results, putting the resulting tibble in a variable called `two_t_unequal_tidy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-consultancy",
   "metadata": {
    "deletable": false,
    "lines_to_next_cell": 2,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49a000fe9a57caa4ff2f9374a99a6cf3",
     "grade": false,
     "grade_id": "cell-44582ffde9fd6881",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# two_t_unequal_tidy <- t.test(... ~ ..., data = attitude_two, \n",
    "#                              var.equal = ...) %>% \n",
    "#            tidy()\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "\n",
    "print(two_t_unequal_tidy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-ceramic",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01db4c8604483800dd3777f7fd1fc2b9",
     "grade": true,
     "grade_id": "cell-5a9a2437322fcb5d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_3.2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-aruba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d6c693fd514f8e1e3ce2a6c91cec31c",
     "grade": false,
     "grade_id": "cell-2cbc678f144f9f51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 3.3**\n",
    "<br>{points: 3}\n",
    "\n",
    "Test the hypothesis using an ANOVA F-test. Use the `tidy()` function from the broom package to tidy your results, putting the resulting tibble in a variable called `two_anova_tidy`. Hint: use the `aov()` function. Scaffolding is given in the worksheet if you need a reminder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-grenada",
   "metadata": {
    "deletable": false,
    "lines_to_next_cell": 2,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33263fc6bde264047841490121f827c4",
     "grade": false,
     "grade_id": "cell-8fb3318a04726b48",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# two_anova_tidy <- ...\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "\n",
    "print(two_anova_tidy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-draft",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 0,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c406220668d6039f07e44b9bb156425a",
     "grade": true,
     "grade_id": "cell-2bf2e0457de039cc",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Here we check to see if you have given your answer the correct object name\n",
    "# and if your answer is plausible. However, all other tests have been hidden\n",
    "# so you can practice deciding when you have the correct answer.\n",
    "\n",
    "test_that('Did not assign answer to an object called \"two_anova_tidy\"', {\n",
    "  expect_true(exists(\"two_anova_tidy\"))\n",
    "})\n",
    "\n",
    "test_that(\"Solution should be a tbl\", {\n",
    "        expect_true(\"tbl\" %in% class(two_anova_tidy))\n",
    "})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-classic",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e6e595e71421126f602e61ec27d6def",
     "grade": false,
     "grade_id": "cell-a0f71eadfc52af1a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 3.4**\n",
    "<br>{points: 1}\n",
    "\n",
    "Store the $p$-values of the above three tests in the following variables. \n",
    "\n",
    "- Two-sample $t$-test, equal variance: `two_t_equal`\n",
    "- Two-sample $t$-test, unequal variance: `two_t_unequal`\n",
    "- ANOVA $F$-test: `two_anova`\n",
    "\n",
    "By the way, the ANOVA $F$-test has a $p$-value that's identical to one of the two $t$-tests (take a look at which one) -- this is no accident, this is always true whenever there are two groups! So, the ANOVA $F$-test is truly a generalization of the two-sample $t$-test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-thirty",
   "metadata": {
    "deletable": false,
    "lines_to_next_cell": 2,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f02a5d7b72f3760e01fca3e7265dd5b9",
     "grade": false,
     "grade_id": "cell-433541c893203176",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# two_t_equal <- ...\n",
    "# two_t_unequal <- ...\n",
    "# two_anova <- ...\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "\n",
    "print(two_t_equal)\n",
    "print(two_t_unequal)\n",
    "print(two_anova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-amazon",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 2,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0bd619abb223525ad6256b4093d1970",
     "grade": true,
     "grade_id": "cell-691ab1b3ef7b80b6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_3.4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-thousand",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9328210d35c1d36e35479c73bade3d77",
     "grade": false,
     "grade_id": "cell-bf5ceafae308d426",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question 3.5**\n",
    "<br>{points: 3}\n",
    "\n",
    "In the above two-sample comparison, the two questions that appeared to have the most similar means were selected for comparison. Why is this bad practice?\n",
    "\n",
    "**a**\\.  If we want to highlight two groups, we should choose the ones that are most _different_, not the most similar.    \n",
    "**b**\\.  This is an example of \"cherry picking\": only presenting two groups that appear the most similar means that these two groups are more likely to be artificially similar.     \n",
    "**c**\\.  This is actually not bad practice at all -- it's a natural next step after ANOVA to investigate pairs of groups. \n",
    "\n",
    "Store the letter of your choice in the variable named `why_inappropriate`. The answer should be a single character surrounded by quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-leather",
   "metadata": {
    "deletable": false,
    "lines_to_next_cell": 2,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93123c2807ac8a3702f8a531bf480961",
     "grade": false,
     "grade_id": "cell-1b4a4dab3f72de55",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# why_inappropriate <- ...\n",
    "\n",
    "# your code here\n",
    "fail() # No Answer - remove if you provide an answer\n",
    "\n",
    "print(why_inappropriate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-planning",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a864f7d94af7967375cc3f2f4da20683",
     "grade": true,
     "grade_id": "cell-76f531f9a0a22509",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Here we check to see if you have given your answer the correct object name\n",
    "# and if your answer is plausible. However, all other tests have been hidden\n",
    "# so you can practice deciding when you have the correct answer.\n",
    "\n",
    "test_that('Did not assign answer to an object called \"why_inappropriate\"', {\n",
    "  expect_true(exists(\"why_inappropriate\"))\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "docker": {
   "latest_image_tag": "v0.6.0"
  },
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
